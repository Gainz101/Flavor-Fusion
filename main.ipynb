{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yezenhijazin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yezenhijazin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import google.generativeai as palm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from spellchecker import SpellChecker  # Import the SpellChecker\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('all_recipies.csv')#imports the dataset\n",
    "#the dataset holds recipe_name,rating,reviews,description,published_date,prep_time,cook_time,total_time,servings,calories,fat,carbs,protein\n",
    "\n",
    "#we focus on the name of the dish and the description however so we pull those out here\n",
    "food_types = df['recipe_name'].tolist()#turns them into a list from that column\n",
    "food_decr_raw = df['description'].tolist()\n",
    "\n",
    "# Download the stopwords from NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#function that preprocesses each description (lowercasing, tokenizing, etc)\n",
    "def preprocess(description):\n",
    "    tokens = word_tokenize(description)\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    postprocessed_desc = [w.lower() for w in tokens if not w.lower() in stop_words]\n",
    "    return \" \".join(postprocessed_desc)\n",
    "\n",
    "food_decr = [preprocess(desc) for desc in food_decr_raw]#a list of processes food discreptions all ready to use\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=[word_tokenize(desc) for desc in food_decr], vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Word2Vec embeddings to initialize Doc2Vec model\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(food_decr)]\n",
    "doc2vec_model = Doc2Vec(vector_size=50, window=5, min_count=1, workers=4, epochs=100)\n",
    "doc2vec_model.build_vocab(tagged_data)\n",
    "doc2vec_model.wv = word2vec_model.wv  # have the doc2vec use Word2Vec embeddings\n",
    "doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orangee chicen', 'calzone', 'humus', 'shawarma', 'hufeoe']\n",
      "The most similar food item to the combined dining history of all users is: Ta'ameya (Egyptian Falafel)\n"
     ]
    }
   ],
   "source": [
    "dining_history=[]\n",
    "dining_histories=[]\n",
    "users=input(\"How many people are in your party? \")\n",
    "for i in range(int(users)):#takes in the dining history of your party\n",
    "    for j in range(0,5):\n",
    "        dining_history.append(input(\"Enter 5 food you've ate recentely or that you like for user \"+str(i+1)+\": \"))\n",
    "    dining_histories.append(dining_history)\n",
    "    dining_history=[]\n",
    "\n",
    "\n",
    "# Create a list of dining histories for multiple users\n",
    "# dining_histories = [[\"lasanga\", \"pesto pasta\", \"pizza\",\"gnocchi\",\"calzone\"],[\"sushi\",\"sashimi\",\"ramen\",\"udon\",\"fish\"]]\n",
    "\n",
    "# Combine all dining histories into one\n",
    "combined_dining_history=[item for sublist in dining_histories for item in sublist]\n",
    "spell = SpellChecker()\n",
    "corrected_combined_dining_history = [spell.correction(word) if spell.correction(word) is not None else word for word in combined_dining_history]\n",
    "\n",
    "\n",
    "# Get the document vector for the combined dining history\n",
    "combined_vector = doc2vec_model.infer_vector(word_tokenize(\" \".join(corrected_combined_dining_history).lower()))\n",
    "\n",
    "#compare our vector with all of the vectors in the model of food descriptions to find the most similar dish for the party/you\n",
    "similar_documents=doc2vec_model.dv.most_similar([combined_vector], topn=1)\n",
    "similar_food_index=int(similar_documents[0][0])\n",
    "similar_food=food_types[similar_food_index]\n",
    "\n",
    "# Calculate cosine similarity between the combined vector and all food vectors\n",
    "# similarities = [cosine_similarity([combined_vector], [doc2vec_model.dv[i]])[0][0] for i in range(len(food_types))]\n",
    "\n",
    "# # Find the index of the most similar food item\n",
    "# similar_food_index = similarities.index(max(similarities))\n",
    "# similar_food = food_types[similar_food_index]\n",
    "\n",
    "print(f\"The most similar food item to the combined dining history of all users is: {similar_food}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian\n",
      "Name: Cento\n",
      "Address: 595 S 3rd St, Columbus, OH 43215\n",
      "---\n",
      "Name: Lola & Giuseppe's Trattoria\n",
      "Address: 100 Granville St, Columbus, OH 43230\n",
      "---\n",
      "Name: La Tavola\n",
      "Address: 1664 W 1st Ave, Grandview Heights, OH 43212\n",
      "---\n",
      "Name: Speck Italian Eatery\n",
      "Address: 89 N High St, Columbus, OH 43215\n",
      "---\n",
      "Name: Marcella's\n",
      "Address: 615 N High St, Columbus, OH 43215\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "palm.configure(api_key= \"AIzaSyCMo4_bDMpu-GArtcs5T5S4rrlfMshnSwg\")#API key for PaLM\n",
    "\n",
    "models = [\n",
    "    m for m in palm.list_models() if \"generateText\" in m.supported_generation_methods\n",
    "]\n",
    "\n",
    "model = models[0].name\n",
    "\n",
    "#the prompt matters alot and had to be fine tined to get the best possible response\n",
    "# prompt = \"Given the food item \"+similar_food+\" suggest a broad genre or type of cuisine that matches this preference. Additionally, provide recommendations for restaurants in College Station, TX within this genre.\" \n",
    "prompt = \"Given the food item \"+similar_food+\" suggest a broad genre or type of cuisine that matches this preference.\"\n",
    "\n",
    "\n",
    "completion = palm.generate_text(\n",
    "    model = model,\n",
    "    prompt=prompt,\n",
    "    temperature=0.33,\n",
    "    # max length of the response\n",
    "    max_output_tokens=800,\n",
    "    \n",
    ")\n",
    "\n",
    "print(completion.result)#prints the prompt response\n",
    "\n",
    "import requests\n",
    "\n",
    "def search_yelp(api_key, term, location, limit=5):\n",
    "  \n",
    "    endpoint = \"https://api.yelp.com/v3/businesses/search\"\n",
    "\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    \n",
    "    params = {\n",
    "        \"term\": term,\n",
    "        \"location\": location,\n",
    "        \"limit\": limit\n",
    "    }\n",
    "\n",
    "  \n",
    "    response = requests.get(endpoint, headers=headers, params=params)\n",
    "\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        \n",
    "        data = response.json()\n",
    "\n",
    "        \n",
    "        for business in data.get(\"businesses\", []):\n",
    "            print(f\"Name: {business['name']}\")\n",
    "            print(f\"Address: {', '.join(business['location']['display_address'])}\")\n",
    "            print(\"---\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    api_key = \"_kpunRxiPhTA4sySgs432sZPBS9afRrsPNYnMOIC66GSuhuadaPuHZfxWiQiu821pjAWVAgJNJ2OEwZjdkbeuphkLzrWEHvWsDFi_gJN7SC-GOn7t6ffJpdWBmFmZXYx\"\n",
    "\n",
    "    \n",
    "    search_term = completion.result\n",
    "    location = \"College Station, TX\"\n",
    "\n",
    "    \n",
    "    limit = 5 \n",
    "\n",
    "    search_yelp(api_key, search_term, location, limit)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
